{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Local\\imgs\\U1\\banner_fcd.jpg\" alt=\"banner\" width=\"1100\"  height=\"150\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:black;\"><strong>√âtica en la Toma de Decisiones Basada en Datos</strong></span> \n",
    "---\n",
    "<p align=\"right\">\n",
    "  <a href=\"https://colab.research.google.com/github/usuario/repositorio/blob/main/ruta/al/notebook.ipynb\" target=\"_blank\">\n",
    "    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Abrir en Colab\"/>\n",
    "  </a>\n",
    "</p>\n",
    "\n",
    "## <span style=\"color:#2F749F;\"><strong>üéØ Objetivos de aprendizaje</strong></span>\n",
    "\n",
    "> ‚úÖ Comprender los **principios √©ticos** que deben guiar la toma de decisiones basada en datos.  \n",
    ">\n",
    "> ‚úÖ Reconocer y mitigar **sesgos y riesgos** en modelos y procesos anal√≠ticos.  \n",
    ">\n",
    "> ‚úÖ Evaluar el impacto de las decisiones basadas en datos en **personas, comunidades y organizaciones**.  \n",
    ">\n",
    "> ‚úÖ Aplicar pr√°cticas de **transparencia, privacidad y responsabilidad** en proyectos de ciencia de datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2F749F;\"><strong>1. Principios √©ticos transversales</strong></span>\n",
    "\n",
    "Estos marcos se sustentan en principios comunes que deben guiar el uso **√©tico y responsable** de los datos:\n",
    "\n",
    "- <span style=\"color:#4CAF50;\">**‚úî Consentimiento informado**</span>: La recolecci√≥n de datos debe basarse en el consentimiento libre, informado y expl√≠cito del titular.  \n",
    "- <span style=\"color:#2196F3;\">**‚úî Transparencia y trazabilidad**</span>: Las personas deben conocer qu√© datos se recopilan, su finalidad, uso y tiempo de conservaci√≥n.  \n",
    "- <span style=\"color:#FFC107;\">**‚úî Minimizaci√≥n y proporcionalidad**</span>: Recoger √∫nicamente los datos estrictamente necesarios para el objetivo declarado.  \n",
    "- <span style=\"color:#FF9800;\">**‚úî Limitaci√≥n de finalidad**</span>: Prohibici√≥n de usar los datos para fines distintos a los autorizados.  \n",
    "- <span style=\"color:#9C27B0;\">**‚úî Seguridad y confidencialidad**</span>: Proteger la informaci√≥n contra accesos no autorizados, p√©rdidas o alteraciones indebidas.  \n",
    "- <span style=\"color:#E91E63;\">**‚úî No discriminaci√≥n y justicia algor√≠tmica**</span>: Evitar que los sistemas predictivos refuercen sesgos o exclusiones sociales.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2F749F;\"><strong>2. Buenas pr√°cticas para el cumplimiento √©tico-normativo</strong></span>\n",
    "\n",
    "- **Privacy by Design** ‚Üí Integrar la privacidad desde la fase inicial del proyecto, no como a√±adido posterior.  \n",
    "- **Evaluaciones de Impacto (DPIA)** ‚Üí Analizar riesgos para la privacidad antes de implementar soluciones.  \n",
    "- **Documentaci√≥n y trazabilidad** ‚Üí Mantener un registro claro de c√≥mo, d√≥nde y por qu√© se usan los datos.  \n",
    "- **Protecciones t√©cnicas** ‚Üí Aplicar *seudonimizaci√≥n*, *differential privacy* y control de acceso granular.  \n",
    "- **Gobernanza √©tica** ‚Üí Establecer auditor√≠as y comit√©s de revisi√≥n multidisciplinarios.  \n",
    "- **Empoderamiento del usuario** ‚Üí Ofrecer herramientas para que consulten, modifiquen o eliminen sus datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2F749F;\"><strong>3. üéØ Sesgo (bias) </strong></span>\n",
    "\n",
    "Un **sesgo** se refiere a una distorsi√≥n sistem√°tica en la recolecci√≥n, an√°lisis, interpretaci√≥n o modelado de datos. Puede ser introducido en varias etapas del ciclo de vida del dato y del modelo. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"Local\\imgs\\U1\\bias.png\" alt=\"bias\" width=\"400\"  height=\"400\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "> üìå En modelos de aprendizaje autom√°tico, el sesgo ocurre cuando las predicciones tienden a desviarse consistentemente del valor real, como si el modelo estuviera ‚Äúapuntando mal‚Äù."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:#2F749F;\"><strong>3.1. Tipos de sesgo </strong></span>\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"Local\\imgs\\U1\\tipos_sesgo.png\" alt=\"tipos-sesgo\" width=\"1000\"  height=\"800\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#2F749F;\"><strong>3.1.1. Muestreo (Sampling Bias) </strong></span>\n",
    "Este sesgo ocurre cuando el procedimiento de selecci√≥n de datos introduce una **representaci√≥n no equilibrada** de la poblaci√≥n objetivo, lo que puede derivar en estimaciones distorsionadas y conclusiones err√≥neas. Este sesgo surge t√≠picamente por la exclusi√≥n sistem√°tica de ciertos subgrupos o la sobrerrepresentaci√≥n de otros debido a m√©todos de recolecci√≥n no aleatorios o mal dise√±ados. \n",
    "\n",
    "Por ejemplo, encuestas realizadas √∫nicamente en entornos urbanos podr√≠an subestimar las perspectivas de poblaciones rurales, generando una falta de validez externa. \n",
    "\n",
    "Minimizar este sesgo requiere un dise√±o muestral representativo y estrategias de estratificaci√≥n o aleatorizaci√≥n adecuadas ([Bethlehem, 2010](https://doi.org/10.1111/j.1751-5823.2010.00112.x); [Lohr, 2021](https://doi.org/10.1201/9781003223713)).\n",
    "\n",
    "> **Ejemplo:** Un modelo predictivo entrenado √∫nicamente con datos de adultos urbanos puede fallar al aplicarse en adolescentes, ya que sus patrones de comportamiento y contexto son distintos.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"Local\\imgs\\U1\\sampling-bias.png\" alt=\"sampling-bias\" width=\"600\"  height=\"500\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#2F749F;\"><strong>3.1.2. Medici√≥n (Measurement Bias) </strong></span>\n",
    "\n",
    "Es un error sistem√°tico que ocurre cuando los datos recopilados **no representan con precisi√≥n** la realidad que se pretende medir. Este sesgo puede surgir por fallas en los instrumentos de medici√≥n, inconsistencias en los procedimientos de recolecci√≥n, condiciones inadecuadas de observaci√≥n o definiciones imprecisas de las variables. \n",
    "\n",
    "A diferencia del error aleatorio, el measurement bias desplaza consistentemente las mediciones en una direcci√≥n, lo que puede comprometer la validez interna y externa de un estudio. \n",
    "\n",
    "En el contexto de ciencia de datos y aprendizaje autom√°tico, este sesgo puede introducir distorsiones en la fase de entrenamiento, generando modelos menos precisos o incluso discriminatorios ([Fowler, 2014](https://uk.sagepub.com/en-gb/eur/survey-research-methods/book239405); [Groves et al., 2009](https://www.wiley.com/en-us/Survey+Methodology%2C+2nd+Edition-p-9780470465462)).\n",
    "\n",
    "> **Ejemplo:** En medicina, si un dispositivo mide sistem√°ticamente la presi√≥n arterial m√°s baja de lo que realmente es, todos los pacientes podr√≠an ser subestimados en su riesgo cardiovascular, afectando decisiones cl√≠nicas basadas en esos datos.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"Local\\imgs\\U1\\measurement_bias.jpg\" alt=\"measurement_bias\" width=\"1000\"  height=\"500\"/>\n",
    "</p>\n",
    "\n",
    "Imagen tomada de [cut-the-saas.com](https://www.cut-the-saas.com/learn-prompting-ai-bias)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#2F749F;\"><strong>3.1.3. Selecci√≥n (Selection Bias) </strong></span>\n",
    "\n",
    "El sesgo de selecci√≥n se presenta cuando la muestra utilizada para entrenar o evaluar un modelo predictivo **no es representativa de la poblaci√≥n objetivo**, lo que conduce a estimaciones y predicciones sistem√°ticamente distorsionadas. Este problema puede surgir en distintas etapas del flujo de datos, como en la recolecci√≥n, muestreo o filtrado, y se manifiesta en contextos donde ciertos grupos o casos tienen mayor probabilidad de ser incluidos que otros. \n",
    "\n",
    "En ciencia de datos y aprendizaje autom√°tico, este sesgo puede provocar que un modelo generalice de forma deficiente, especialmente cuando se aplica a datos fuera de la distribuci√≥n con la que fue entrenado ([Hastie, Tibshirani, & Friedman, 2009](https://link.springer.com/book/10.1007/978-0-387-84858-7); [Kohavi et al., 2020](https://link.springer.com/article/10.1007/s10618-008-0114-1)).\n",
    "\n",
    "> Se distinguen **varios tipos** espec√≠ficos de sesgo de selecci√≥n, entre ellos:\n",
    "> - Sesgo por muestreo no aleatorio: la muestra se obtiene de manera que excluye sistem√°ticamente parte de la poblaci√≥n.\n",
    ">\n",
    "> - Sesgo por autoselecci√≥n: los individuos se incluyen o excluyen seg√∫n decisiones propias (p. ej., encuestas voluntarias).\n",
    ">\n",
    "> - Sesgo de supervivencia: se consideran solo los casos que \"sobreviven\" a cierto filtro temporal o condicional.\n",
    ">\n",
    "> - Sesgo de atrici√≥n: p√©rdida sistem√°tica de participantes a lo largo del tiempo.\n",
    "\n",
    "En la pr√°ctica, mitigar este sesgo requiere t√©cnicas como muestreo estratificado, ponderaci√≥n estad√≠stica, validaci√≥n con datos externos y, en contextos de machine learning, el uso de domain adaptation para ajustar modelos a cambios de distribuci√≥n ([Zhao et al., 2022](https://dl.acm.org/doi/10.1145/3527152)).\n",
    "\n",
    "La prevenci√≥n requiere un dise√±o muestral riguroso, estrategias para maximizar la respuesta y m√©todos estad√≠sticos que ajusten las diferencias observadas ([Hern√°n, Hern√°ndez-D√≠az & Robins, 2004](https://journals.lww.com/epidem/fulltext/2004/09000/a_structural_approach_to_selection_bias.20.aspx); [Delgado-Rodr√≠guez & Llorca, 2004](https://jech.bmj.com/content/58/8/635)).\n",
    "\n",
    "\n",
    "> **Ejemplo:** Evaluar la eficacia de un tratamiento m√©dico solo con pacientes que completaron el tratamiento, ignorando a quienes lo abandonaron, puede sobreestimar su efectividad real.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"Local\\imgs\\U1\\selection-bias.png\" alt=\"selection-bias\" width=\"800\"  height=\"500\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#2F749F;\"><strong>3.1.4. Algor√≠tmico (Algorithmic Bias)</strong></span>\n",
    "El sesgo algor√≠tmico se refiere a la generaci√≥n sistem√°tica de **resultados distorsionados** por parte de un sistema de inteligencia artificial o aprendizaje autom√°tico, debido a la influencia de prejuicios impl√≠citos presentes en los datos, en el dise√±o del algoritmo o en las decisiones de modelado. Este fen√≥meno ocurre incluso cuando los datos aparentan ser neutrales, ya que los algoritmos pueden amplificar y perpetuar desigualdades sociales, hist√≥ricas o institucionales existentes. \n",
    "\n",
    "Este tipo de sesgo puede **originarse en diferentes etapas del ciclo de vida del sistema**, incluyendo la selecci√≥n de variables, la recolecci√≥n de datos, la definici√≥n de objetivos de optimizaci√≥n o el uso de m√©tricas que no capturan adecuadamente la equidad ([Barocas, Hardt & Narayanan, 2023](https://fairmlbook.org/); [Mehrabi et al., 2021](https://dl.acm.org/doi/10.1145/3457607)). \n",
    "\n",
    "En consecuencia, los sesgos algor√≠tmicos no solo afectan la precisi√≥n y la validez de los modelos, sino que tambi√©n tienen implicaciones √©ticas y sociales de gran alcance, especialmente en contextos sensibles como la salud, la justicia y el acceso a recursos.\n",
    "\n",
    "> **Ejemplo:** Un sistema de recomendaci√≥n de empleo que hist√≥ricamente ha priorizado candidatos hombres para roles t√©cnicos puede seguir haci√©ndolo, incluso si las mujeres tienen perfiles equivalentes, perpetuando la discriminaci√≥n de g√©nero.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"Local\\imgs\\U1\\algorithmic-bias.jpg\" alt=\"algorithmic-bias\" width=\"1000\"  height=\"500\"/>\n",
    "</p>\n",
    "\n",
    "Tomado de [alumni.berkeley.edu](https://alumni.berkeley.edu/california-magazine/online/biased-algorithms-exacerbate-racial-inequality-health-care/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#2F749F;\"><strong>3.1.5. Por retroalimentaci√≥n (Feedback Loop)</strong></span>\n",
    "\n",
    "Este sesgo se produce cuando **las predicciones** generadas por un modelo **influyen en las decisiones o comportamientos** que, a su vez, afectan los datos que se recopilan posteriormente y que ser√°n utilizados para reentrenar el mismo modelo.\n",
    "\n",
    "Este ciclo cerrado **puede reforzar y amplificar patrones preexistentes** ‚Äîincluidos aquellos incorrectos, incompletos o discriminatorios‚Äî, provocando que los errores sistem√°ticos se acumulen con el tiempo. \n",
    "\n",
    "En contextos como la asignaci√≥n de recursos, la evaluaci√≥n de riesgos o los sistemas de recomendaci√≥n, este fen√≥meno puede consolidar desigualdades estructurales y reducir la capacidad del modelo para generalizar de manera justa y precisa ([Ensign et al., 2018](https://arxiv.org/abs/1706.09847); [Mehrabi et al., 2021](https://dl.acm.org/doi/10.1145/3457607)).\n",
    "\n",
    "> **Ejemplo:** Un modelo policial que predice mayor criminalidad en ciertas zonas puede llevar a asignar m√°s patrullas all√≠. Esto genera m√°s reportes y arrestos en esa √°rea, reforzando la creencia de que hay m√°s criminalidad, aunque no se est√© midiendo objetivamente.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"Local\\imgs\\U1\\algorithmic-bias.jpg\" alt=\"algorithmic-bias\" width=\"1000\"  height=\"500\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:#2F749F;\"><strong>3.1.6. Resumen de tipos de sesgo</strong></span>\n",
    "\n",
    "A continuaci√≥n, se realiza una comparaci√≥n estructurada de algunos tipos de sesgo identificados en la literatura, considerando su definici√≥n, causas comunes, ejemplos pr√°cticos y estrategias de mitigaci√≥n. \n",
    "\n",
    "| Tipo de sesgo                             | Definici√≥n breve                                                                                                                                                 | Ejemplo sencillo                                                                                                     | Causas principales                                                                                                            | Estrategias de mitigaci√≥n                                                                                                    |\n",
    "| ----------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Muestreo (Sampling Bias)**              | Sesgo que surge cuando la muestra de datos no representa adecuadamente a la poblaci√≥n objetivo, debido a exclusi√≥n o sobrerrepresentaci√≥n sistem√°tica de grupos. | Modelo de salud entrenado solo con datos de adultos urbanos aplicado a poblaci√≥n rural adolescente.                  | Selecci√≥n no aleatoria, subrepresentaci√≥n de grupos minoritarios, limitaciones geogr√°ficas o temporales.                     | Muestreo aleatorio estratificado, aumento de la diversidad de la muestra, t√©cnicas de ponderaci√≥n.                           |\n",
    "| **Medici√≥n (Measurement Bias)**           | Ocurre cuando los datos recopilados no reflejan con precisi√≥n la realidad, por errores en instrumentos, inconsistencias en recolecci√≥n o definiciones ambiguas.  | Sensor de temperatura mal calibrado que registra fiebre en pacientes sanos.                                          | Instrumentos defectuosos, protocolos inconsistentes, errores humanos, definiciones vagas.                                    | Calibrar y validar instrumentos, estandarizar procesos de recolecci√≥n, capacitaci√≥n del personal.                            |\n",
    "| **Selecci√≥n (Selection Bias)**            | Aparece al incluir o excluir datos de manera que distorsionan las conclusiones, ya sea por criterios expl√≠citos o por disponibilidad limitada de la informaci√≥n. | Estudio de productividad que omite trabajadores remotos, sobreestimando horas presenciales como indicador clave.     | Inclusi√≥n o exclusi√≥n sesgada, limitaciones de acceso a datos, filtrado intencional o accidental de casos.                   | Ampliar las fuentes de datos, criterios de inclusi√≥n claros y transparentes, auditor√≠as peri√≥dicas de la base de datos.      |\n",
    "| **Algor√≠tmico (Algorithmic Bias)**        | Se produce cuando un modelo reproduce o amplifica desigualdades hist√≥ricas, sociales o institucionales presentes en los datos o en el dise√±o del algoritmo.      | Uso del historial de arrestos como predictor de ‚Äúriesgo‚Äù, penalizando m√°s a minor√≠as raciales.                       | Datos de entrenamiento con sesgos hist√≥ricos, dise√±o de variables problem√°ticas, objetivos de optimizaci√≥n no inclusivos.    | Revisi√≥n √©tica de variables, recolecci√≥n de datos m√°s diversos, ajuste de la funci√≥n objetivo para promover equidad.         |\n",
    "| **Por retroalimentaci√≥n (Feedback Loop)** | Sesgo en el que las predicciones del modelo afectan los datos futuros que lo alimentan, reforzando patrones iniciales y amplificando errores o desigualdades.    | Sistema policial que asigna m√°s patrullas a zonas ‚Äúde riesgo‚Äù detectadas por el modelo, generando m√°s arrestos all√≠. | Implementaci√≥n de modelos en contextos donde sus resultados influyen directamente en las futuras observaciones.               | Monitoreo continuo del impacto, redise√±o peri√≥dico del modelo, uso de datos externos para romper el ciclo de retroalimentaci√≥n.|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#2F749F;\"><strong>3.2. ¬øC√≥mo detectar sesgos? </strong></span>\n",
    "\n",
    "Detectar sesgos es un paso esencial para garantizar decisiones automatizadas justas, transparentes y socialmente responsables. A continuaci√≥n se presentan m√©todos clave para identificar sesgos en sistemas algor√≠tmicos:\n",
    "\n",
    "### <span style=\"color:#2F749F;\"><strong>3.2.1. Exploraci√≥n inicial de los datos</strong></span>\n",
    "- Analiza distribuciones por atributos sensibles como **g√©nero**, **raza/etnia**, **edad**, **nivel socioecon√≥mico**, etc.\n",
    "- Revisa valores ausentes, codificaciones problem√°ticas y posibles correlaciones con variables de salida.\n",
    "- Visualizaciones recomendadas:\n",
    "  - Histogramas comparativos\n",
    "  - Gr√°ficos de barras segmentados\n",
    "  - Boxplots por grupo\n",
    "\n",
    "### <span style=\"color:#2F749F;\"><strong>3.2.2. Comparaci√≥n de m√©tricas por subgrupos</strong></span>\n",
    "- Eval√∫a el rendimiento del modelo (precisi√≥n, recall, F1-score, AUC) por cada grupo demogr√°fico.\n",
    "- Identifica disparidades significativas en errores (falsos positivos/negativos) entre subpoblaciones.\n",
    "- Aplica m√©tricas de equidad como:\n",
    "  - Equal Opportunity\n",
    "  - Demographic Parity\n",
    "  - Predictive Equality\n",
    "\n",
    "### <span style=\"color:#2F749F;\"><strong>3.2.3. Uso de herramientas de auditor√≠a algor√≠tmica</strong></span>\n",
    "Estas herramientas permiten auditar, visualizar y mitigar sesgos en sistemas automatizados:\n",
    "\n",
    "| Herramienta                               | Enlace                                                                                      | Caracter√≠sticas Principales                                                                                  | Compatible con...                          |\n",
    "|-------------------------------------------|---------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------|--------------------------------------------|\n",
    "| **Fairness Indicators (Google)**          | [üîó Repositorio GitHub](https://github.com/tensorflow/fairness-indicators)                             | M√©tricas visuales para modelos de clasificaci√≥n y comparaci√≥n entre subgrupos.                              | TensorFlow                                 |\n",
    "| **Aequitas**                               | [üîó Repositorio GitHub](https://github.com/dssg/aequitas)                                               | Auditor√≠a √©tica, con reportes claros sobre disparidades por atributos sensibles.                            | Python, modelos personalizados             |\n",
    "| **AI Fairness 360 (IBM)**                 | [üîó Repositorio GitHub](https://github.com/Trusted-AI/AIF360)                                           | M√©tricas + algoritmos para mitigar sesgos antes, durante y despu√©s del modelado.                            | Python, scikit-learn, TensorFlow           |\n",
    "| **What-If Tool**                          | [üîó P√°gina Web](https://pair-code.github.io/what-if-tool/)                                         | Interfaz visual para modificar datos y analizar impacto en las predicciones sin escribir c√≥digo.            | TensorFlow SavedModel, Cloud AI Platform   |\n",
    "| **Fairlearn**                             | [üîó Repositorio GitHub](https://github.com/fairlearn/fairlearn)                                         | T√©cnicas de mitigaci√≥n + gr√°ficos para an√°lisis de equidad y disparidad en modelos.                         | scikit-learn, Jupyter Notebooks            |\n",
    "| **EthicalML**                             | [üîó Repositorio GitHub](https://github.com/EthicalML)                                                   | Repositorio colaborativo enfocado en transparencia y responsabilidad algor√≠tmica.                           | Multiplataforma, abierta y flexible        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Referencias**\n",
    "\n",
    "- Amorim, E., Can√ßado, M., & Veloso, A. (2018). Automated Essay Scoring in the Presence of Biased Ratings. En: Proceedings of NAACL-HLT 2018.  \n",
    "- Barocas, S., Hardt, M., & Narayanan, A. (2019). *Fairness and Machine Learning*.  \n",
    "- Biblioteca del Congreso Nacional de Chile. (2020). *Ley sobre protecci√≥n de la vida privada*.  \n",
    "  [https://www.bcn.cl/leychile/navegar?idNorma=141599](https://www.bcn.cl/leychile/navegar?idNorma=141599)\n",
    "- Danks, D., & London, A. J. (2017). Algorithmic bias in autonomous systems. *IJCAI*, 4691‚Äì4697.  \n",
    "- Doshi-Velez, F., & Kim, B. (2017). Towards A Rigorous Science of Interpretable Machine Learning. \n",
    "- Floridi, L., & Taddeo, M. (2016). *What is data ethics?*. _Philosophical Transactions of the Royal Society A_, 374(2083).  \n",
    "  [https://doi.org/10.1098/rsta.2016.0360](https://doi.org/10.1098/rsta.2016.0360)\n",
    "- Ley 1581 de 2012. (Colombia). *Por la cual se dictan disposiciones generales para la protecci√≥n de datos personales*. Superintendencia de Industria y Comercio.  \n",
    "  [https://www.sic.gov.co/proteccion-de-datos-personales](https://www.sic.gov.co/proteccion-de-datos-personales)\n",
    "- Litman, D., Zhang, H., Correnti, R., Matsumura, L. C., & Wang, E. (2021). A Fairness Evaluation of Automated Methods for Scoring Text Evidence Usage in Writing. En: Artificial Intelligence in Education (AIED 2021), Lecture Notes in Computer Science, vol. 12748\n",
    "- Lundberg, S. M., & Lee, S.-I. (2017). A Unified Approach to Interpreting Model Predictions. NeurIPS. SHAP Paper\n",
    "- Mittelstadt, B. D., Allo, P., Taddeo, M., Wachter, S., & Floridi, L. (2016). *The ethics of algorithms: Mapping the debate*. _Big Data & Society_, 3(2). [https://doi.org/10.1177/2053951716679679](https://doi.org/10.1177/2053951716679679)\n",
    "- Obermeyer, Z., et al. (2019). Dissecting racial bias in an algorithm used to manage the health of populations. *Science*, 366(6464), 447‚Äì453.\n",
    "- Ribeiro, M. T., Singh, S., & Guestrin, C. (2016). \"Why Should I Trust You?\": Explaining the Predictions of Any Classifier. KDD. DOI:10.1145/2939672.2939778\n",
    "- Suresh, H., & Guttag, J. V. (2019). A Framework for Understanding Unintended Consequences of Machine Learning. Communications of the ACM.\n",
    "- Wachter, S., Mittelstadt, B., & Floridi, L. (2017). *Why a right to explanation of automated decision-making does not exist in the General Data Protection Regulation*. _International Data Privacy Law_, 7(2), 76‚Äì99.  \n",
    "  [https://doi.org/10.1093/idpl/ipx005](https://doi.org/10.1093/idpl/ipx005)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<div style=\"text-align:center; color:#607D8B; font-weight:bold; font-size:14px;\">\n",
    "‚ÄúSin datos, solo eres otra persona con una opini√≥n.‚Äù ‚Äì W. Edwards Deming\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
